{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703b6bcf-bf53-4798-95b1-6bb991456b2e",
   "metadata": {},
   "source": [
    "**Final Project**\n",
    "\n",
    "Kevin Gardenhire (keving8)\n",
    "\n",
    "Michael Foster ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eae270af-60a7-4f2a-87ee-0ce3e29b80a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "pandas==2.2.3\n",
    "seaborn==0.13.2\n",
    "nltk==3.9.1\n",
    "iteration-utilities==0.13.0\n",
    "scikit-learn==1.6.1\n",
    "torch>=2.0.0\n",
    "numpy>=2.2\n",
    "transformers>=4.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7484f281-cd35-4d31-9d3a-78c4f8db8109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.2.3 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: seaborn==0.13.2 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.13.2)\n",
      "Requirement already satisfied: nltk==3.9.1 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (3.9.1)\n",
      "Requirement already satisfied: iteration-utilities==0.13.0 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.13.0)\n",
      "Requirement already satisfied: scikit-learn==1.6.1 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (1.6.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (2.2.2)\n",
      "Requirement already satisfied: numpy>=2.2 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (2.2.5)\n",
      "Requirement already satisfied: transformers>=4.51 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (4.51.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from pandas==2.2.3->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from pandas==2.2.3->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from pandas==2.2.3->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from seaborn==0.13.2->-r requirements.txt (line 3)) (3.10.1)\n",
      "Requirement already satisfied: click in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from nltk==3.9.1->-r requirements.txt (line 4)) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from nltk==3.9.1->-r requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from nltk==3.9.1->-r requirements.txt (line 4)) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from nltk==3.9.1->-r requirements.txt (line 4)) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from scikit-learn==1.6.1->-r requirements.txt (line 6)) (1.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from scikit-learn==1.6.1->-r requirements.txt (line 6)) (3.6.0)\n",
      "Requirement already satisfied: filelock in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from torch>=2.0.0->-r requirements.txt (line 7)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from torch>=2.0.0->-r requirements.txt (line 7)) (4.13.2)\n",
      "Requirement already satisfied: sympy in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from torch>=2.0.0->-r requirements.txt (line 7)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from torch>=2.0.0->-r requirements.txt (line 7)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from torch>=2.0.0->-r requirements.txt (line 7)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from torch>=2.0.0->-r requirements.txt (line 7)) (2025.3.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from transformers>=4.51->-r requirements.txt (line 9)) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from transformers>=4.51->-r requirements.txt (line 9)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from transformers>=4.51->-r requirements.txt (line 9)) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from transformers>=4.51->-r requirements.txt (line 9)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from transformers>=4.51->-r requirements.txt (line 9)) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from transformers>=4.51->-r requirements.txt (line 9)) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2->-r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2->-r requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2->-r requirements.txt (line 3)) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2->-r requirements.txt (line 3)) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2->-r requirements.txt (line 3)) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2->-r requirements.txt (line 3)) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.2.3->-r requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 7)) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from requests->transformers>=4.51->-r requirements.txt (line 9)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from requests->transformers>=4.51->-r requirements.txt (line 9)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from requests->transformers>=4.51->-r requirements.txt (line 9)) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from requests->transformers>=4.51->-r requirements.txt (line 9)) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/Kevin/miniconda/envs/DL4H_FINAL/lib/python3.10/site-packages (from sympy->torch>=2.0.0->-r requirements.txt (line 7)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a87d109f-e9c8-4f01-8220-24973ad9374b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Kevin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/Kevin/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download NLTK tokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ac4b766-392e-49b6-a6af-ff7dd782066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import csv, json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from iteration_utilities import duplicates, unique_everseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00da8bc0-3e7e-4211-9687-ef459833bbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates EM and F1 scores.\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = {'a','an','the'}\n",
    "\n",
    "\n",
    "def word_extraction(sentence):   \n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split()    \n",
    "    return words\n",
    "\n",
    "\n",
    "def tokenize(sentences):\n",
    "\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        w = word_extraction(sentence)\n",
    "        words.extend(w)\n",
    "    words = sorted(list(set(words)))   \n",
    "    return words\n",
    "\n",
    "\n",
    "def prep(text):\n",
    "    \n",
    "    text = text.replace('\\n', ' ').lower()              # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ',text)            # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('',text)                  # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join([w for w in text.split() if not w in STOPWORDS])# delete stopwords from text\n",
    "    return text\n",
    "\n",
    "\n",
    "def calculate_f1(w1, w2):\n",
    "    \n",
    "    vocab = tokenize([w1, w2])\n",
    "    true_words = word_extraction(w1)\n",
    "    predicted_words = word_extraction(w2)\n",
    "    true_bag_vector = np.zeros(len(vocab))\n",
    "    predicted_bag_vector = np.zeros(len(vocab))\n",
    "    \n",
    "    for w in true_words:\n",
    "        for i, word in enumerate(vocab):\n",
    "            if word == w:\n",
    "                true_bag_vector[i] += 1\n",
    "                \n",
    "    for w in predicted_words:\n",
    "        for i, word in enumerate(vocab):\n",
    "            if word == w:\n",
    "                predicted_bag_vector[i] += 1\n",
    "                \n",
    "    macro_f1 = metrics.f1_score(true_bag_vector, predicted_bag_vector, average = 'macro')\n",
    "    \n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78323eb8-da7e-49de-8c79-1312daf7555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datset analysis Methods\n",
    "#we are calculating:\n",
    "#1. average number of words per paragraph, question, answer\n",
    "#2. calculate entailment for answers and questions\n",
    "#3. calculate similarities between questions and answers\n",
    "#4. draw histograms/pie chart\n",
    "def read_files(file_list):\n",
    "    \n",
    "    q_a = []\n",
    "    \n",
    "    for file in file_list:\n",
    "        with open(file, \"r\", encoding = \"utf-8\") as fin:\n",
    "            for line in csv.reader(fin, delimiter = \"\\t\"):\n",
    "                \n",
    "                q = line[0]\n",
    "                a = line[1].replace('[\"', \"\").replace('\"]', \"\").replace('\"', '') \n",
    "                \n",
    "                q_a.append([q, a])\n",
    "    \n",
    "    return q_a\n",
    "\n",
    "\n",
    "def avg_no_words(file_list, text_corpus):\n",
    "      \n",
    "    no_words_p, no_words_q, no_words_a = [], [], []\n",
    "    first_word, second_word = defaultdict(int), defaultdict(int)\n",
    "    \n",
    "    q_a = read_files(file_list)\n",
    "    \n",
    "    for q, a in q_a:\n",
    "                \n",
    "        words_q = word_tokenize(q)\n",
    "        words_a = word_tokenize(a)\n",
    "        \n",
    "        no_words_q.append(len(words_q))\n",
    "        no_words_a.append(len(words_a))\n",
    "\n",
    "        first_word[words_q[0]] += 1\n",
    "        second_word[words_q[1]] += 1\n",
    "   \n",
    "    with open(text_corpus, \"r\", encoding = \"utf-8\") as fin:\n",
    "        for line in csv.reader(fin, delimiter = \"\\t\"):\n",
    "\n",
    "            p = line[1]         \n",
    "            words_p = word_tokenize(p)          \n",
    "            no_words_p.append(len(words_p))\n",
    "   \n",
    "    print(\"Average number of words in paragraphs: {:.1f}\".format(sum(no_words_p)/len(no_words_p)))   \n",
    "    print(\"Average number of words in questions: {:.1f}\".format(sum(no_words_q)/len(no_words_q)))   \n",
    "    print(\"Average number of words in answers: {:.1f}\".format(sum(no_words_a)/len(no_words_a)))  \n",
    "    \n",
    "    for key in first_word.keys():\n",
    "        print(\"{}: {:.1f}%\".format(key, first_word[key]*100/len(no_words_q)))  \n",
    "    \n",
    "    # draw histograms of par/q/a lenghts    \n",
    "    draw_hist(no_words_p, no_words_q, no_words_a)\n",
    "    draw_pie(second_word)  \n",
    "    \n",
    "        \n",
    "def draw_hist(no_words_p, no_words_q, no_words_a):\n",
    "    \n",
    "    plt.figure(figsize=(7, 7))  \n",
    "    plt.hist(no_words_q, alpha = 0.5, bins = np.arange(25), edgecolor = 'black', label=\"questions\")\n",
    "    plt.hist(no_words_a, alpha = 0.5, bins = np.arange(25), edgecolor = 'black', label=\"answers\")\n",
    "    plt.xlabel('Number of words', fontsize = 18)\n",
    "    plt.ylabel('Number of questions/answers', fontsize = 18)\n",
    "    plt.xticks(fontsize = 15)\n",
    "    plt.yticks(fontsize = 15)\n",
    "    plt.xlim(xmin = 0, xmax = 25)\n",
    "    plt.ylim(ymin = 0, ymax = 750)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.savefig(\"no-words-qa.pdf\", transparent = True, bbox_inches = 'tight')\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(7, 7))  \n",
    "    plt.hist(no_words_p, bins = 25, alpha = 0.5, edgecolor = 'black', color='green')\n",
    "    plt.xlabel('Number of words', fontsize = 18)\n",
    "    plt.ylabel('Number of paragraphs', fontsize = 18)\n",
    "    plt.xticks(fontsize = 15)\n",
    "    plt.yticks(fontsize = 15)\n",
    "    plt.xlim(xmin = 50, xmax = 220)\n",
    "    plt.ylim(ymin = 0, ymax = 2100)\n",
    "    plt.savefig(\"no-words-par.pdf\", transparent = True, bbox_inches = 'tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def draw_pie(second_word):\n",
    " \n",
    "    total, others = 0, 0\n",
    "    values, labels = [], []\n",
    "    for key in dict(sorted(second_word.items(), reverse = True, key = lambda item: item[1])):\n",
    "         # first five most popular words\n",
    "         if(total < 5):\n",
    "             values.append(second_word[key])\n",
    "             labels.append(key)\n",
    "         else:\n",
    "             others += second_word[key]\n",
    "         total += 1\n",
    "    \n",
    "    values.append(others)       \n",
    "    labels.append(\"others\")\n",
    "    \n",
    "    colors = sns.color_palette(\"husl\", 6)\n",
    "    plt.figure(figsize=(9, 9))  \n",
    "    plt.pie(values, labels = labels, colors = colors, autopct = '%.0f%%', textprops = {'fontsize': 18})\n",
    "    plt.savefig(\"second-word-qns.pdf\", transparent = True, bbox_inches = 'tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def calculate_entailment(file_list):\n",
    "  \n",
    "    ent_a, ent_q = defaultdict(set), defaultdict(set)\n",
    "    \n",
    "    q_a = read_files(file_list) \n",
    "    dup_q = list(unique_everseen(duplicates([item[0] for item in q_a])))\n",
    "    dup_a = list(unique_everseen(duplicates([item[1] for item in q_a])))\n",
    "  \n",
    "    for q, a in q_a:\n",
    "         if(a in dup_a):\n",
    "             ent_q[a].add(q)\n",
    "         if(q in dup_q):\n",
    "             ent_a[q].add(a)\n",
    "    \n",
    "    print(\"Number of question entailment: {}\".format(sum([len(list(ent_q[x])) for x in ent_q if isinstance(list(ent_q[x]), list)])))\n",
    "    print(\"Number of answer entailment: {}\".format(sum([len(list(ent_a[x])) for x in ent_a if isinstance(list(ent_a[x]), list)])))\n",
    "   \n",
    "\n",
    "def calculate_qa_sim(test_json):\n",
    "    \n",
    "    counter = 0\n",
    "    f1 = []\n",
    "    \n",
    "    with open(test_json, \"r\", encoding = \"utf-8\") as fin: \n",
    "        objs = json.load(fin)\n",
    "        for obj in objs:\n",
    "            q = obj['question']\n",
    "            a = obj['answers'][0]\n",
    "            \n",
    "            try:\n",
    "                p = obj['positive_ctxs'][0]['text']\n",
    "                \n",
    "                if(p.count(a) == 1):\n",
    "                    counter += 1\n",
    "                   \n",
    "                    if(counter == 1000):\n",
    "                        break\n",
    "                    \n",
    "                    sentences = sent_tokenize(p)\n",
    "                    \n",
    "                    for sentence in sentences:\n",
    "                        if(sentence.find(a) != -1):\n",
    "                            \n",
    "                            macro_f1 = calculate_f1(q, sentence)   \n",
    "                            f1.append(macro_f1)\n",
    "            except:\n",
    "                pass\n",
    "                  \n",
    "    print(\"Similarities between questions and answers (f1): {:.2f}\".format(sum(f1)/len(f1)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1013931f-7ff7-421a-95fe-a6675e66e7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate inter-annotator agreement for label collection process and for extrinsic evaluation.\n",
    "def labels_agreement(agreement_file):\n",
    "    \n",
    "    f1, em = [], []\n",
    "   \n",
    "    a = defaultdict(list)\n",
    "\n",
    "    # main one first\n",
    "    with open(agreement_file, \"r\", encoding = \"utf-8\") as fin:\n",
    "        for line in csv.reader(fin):\n",
    "            # no#answer#is_main\n",
    "            if(line[2] == \"TRUE\"): \n",
    "                a[int(line[0])].append(prep(line[1]))\n",
    "              \n",
    "    # others after\n",
    "    with open(agreement_file, \"r\", encoding = \"utf-8\") as fin:\n",
    "        for line in csv.reader(fin):\n",
    "            if(line[2] == \"FALSE\" and int(line[0]) in a.keys()):\n",
    "                a[int(line[0])].append(prep(line[1]))\n",
    "   \n",
    "    for key in a:\n",
    "       \n",
    "        if(len(a[key]) != 5):\n",
    "            print(a[key])\n",
    "           \n",
    "        max_f1 = -1\n",
    "        for i in range(1, 5):\n",
    "            macro_f1 = calculate_f1(a[key][0], a[key][i]) \n",
    "            if(macro_f1 > max_f1):\n",
    "                max_f1 = macro_f1\n",
    "           \n",
    "        f1.append(max_f1)       \n",
    "        \n",
    "        if(max_f1 == 1):\n",
    "            em.append(1)\n",
    "        else:\n",
    "            em.append(0)\n",
    "   \n",
    "    print(\"em: {:.2f}\".format(sum(em)/len(em)))\n",
    "    print(\"f1: {:.2f}\".format(sum(f1)/len(f1)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56c6b464-8afe-424a-805a-d9a096d5ca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "em: 0.85\n",
      "f1: 0.91\n",
      "Average number of words in paragraphs: 120.6\n",
      "Average number of words in questions: 9.9\n",
      "Average number of words in answers: 10.3\n",
      "what: 68.4%\n",
      "how: 16.8%\n",
      "why: 5.9%\n",
      "who: 2.9%\n",
      "when: 4.9%\n",
      "where: 1.0%\n",
      "Number of question entailment: 222\n",
      "Number of answer entailment: 149\n",
      "Similarities between questions and answers (f1): 0.17\n"
     ]
    }
   ],
   "source": [
    "agreement_file = \"data/agreement/labels_agreement.csv\"\n",
    "    \n",
    "file_list = [\"data/training/sleep-train.csv\", \n",
    "                \"data/training/sleep-dev.csv\", \n",
    "                \"data/training/sleep-test.csv\"]\n",
    "    \n",
    "text_corpus = \"data/training/sleep-corpus.tsv\"\n",
    "    \n",
    "test_json = \"data/training/sleep-train.json\"\n",
    "  \n",
    "# calculate em/f1 for inter-annotators agreement on labels\n",
    "labels_agreement(agreement_file)\n",
    "    \n",
    "# calculate avg. number of words in paragraphs/questions/answers\n",
    "avg_no_words(file_list, text_corpus)\n",
    "    \n",
    "# calculate question/answer entailment\n",
    "calculate_entailment(file_list)\n",
    "    \n",
    "# calculate similarities between questions and answers\n",
    "calculate_qa_sim(test_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f34be0-c5c2-47a0-93fe-6c5ed2d71b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRReaderTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted spans for 10 questions.\n",
      "Extracted spans for 20 questions.\n",
      "Extracted spans for 30 questions.\n",
      "Extracted spans for 40 questions.\n",
      "Extracted spans for 50 questions.\n",
      "Extracted spans for 60 questions.\n",
      "Extracted spans for 70 questions.\n",
      "Extracted spans for 80 questions.\n",
      "Extracted spans for 90 questions.\n",
      "Extracted spans for 100 questions.\n",
      "Extracted spans for 110 questions.\n",
      "Extracted spans for 120 questions.\n",
      "Extracted spans for 130 questions.\n",
      "Extracted spans for 140 questions.\n",
      "Extracted spans for 150 questions.\n",
      "Extracted spans for 160 questions.\n",
      "Extracted spans for 170 questions.\n",
      "Extracted spans for 180 questions.\n",
      "Extracted spans for 190 questions.\n",
      "Extracted spans for 200 questions.\n",
      "Extracted spans for 210 questions.\n",
      "Extracted spans for 220 questions.\n",
      "Extracted spans for 230 questions.\n",
      "Extracted spans for 240 questions.\n",
      "Extracted spans for 250 questions.\n",
      "Extracted spans for 260 questions.\n",
      "Extracted spans for 270 questions.\n",
      "Extracted spans for 280 questions.\n",
      "Extracted spans for 290 questions.\n",
      "Extracted spans for 300 questions.\n",
      "Extracted spans for 310 questions.\n",
      "Extracted spans for 320 questions.\n",
      "Extracted spans for 330 questions.\n",
      "Extracted spans for 340 questions.\n",
      "Extracted spans for 350 questions.\n",
      "Extracted spans for 360 questions.\n",
      "Extracted spans for 370 questions.\n",
      "Extracted spans for 380 questions.\n",
      "Extracted spans for 390 questions.\n",
      "Extracted spans for 400 questions.\n",
      "Extracted spans for 410 questions.\n",
      "Extracted spans for 420 questions.\n",
      "Extracted spans for 430 questions.\n",
      "Extracted spans for 440 questions.\n",
      "Extracted spans for 450 questions.\n",
      "Extracted spans for 460 questions.\n",
      "Extracted spans for 470 questions.\n",
      "Extracted spans for 480 questions.\n",
      "Extracted spans for 490 questions.\n",
      "Extracted spans for 500 questions.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'calculate_f1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 254\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m#generate_dense_encodings(text_corpus, ctx_encoder, corpus_embedded)\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m#dense_retriever(questions, question_encoder, text_corpus, corpus_embedded, retrieved_passages)\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m#validate_retriever(questions, retrieved_passages)\u001b[39;00m\n\u001b[1;32m    252\u001b[0m extractive_reader(retrieved_passages, reader, span_answers)\n\u001b[0;32m--> 254\u001b[0m \u001b[43mvalidate_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspan_answers\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 97\u001b[0m, in \u001b[0;36mvalidate_reader\u001b[0;34m(questions, span_answers)\u001b[0m\n\u001b[1;32m     93\u001b[0m question \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     95\u001b[0m answer \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m macro_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_f1\u001b[49m(qa_dic[question], answer)   \n\u001b[1;32m     98\u001b[0m f1\u001b[38;5;241m.\u001b[39mappend(macro_f1)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(macro_f1 \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'calculate_f1' is not defined"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append(\"../utils\")\n",
    "sys.path.append(\"../DPR-main/\")\n",
    "\n",
    "#from f1_score import calculate_f1\n",
    "\n",
    "import csv, pickle, torch, time\n",
    "from transformers import (\n",
    "    DPRContextEncoder, DPRContextEncoderTokenizer, \n",
    "    DPRQuestionEncoder, DPRQuestionEncoderTokenizer, \n",
    "    DPRReader, DPRReaderTokenizer\n",
    "    )\n",
    "\n",
    "from faiss_indexers import DenseFlatIndexer\n",
    "\n",
    "\n",
    "def iterate_encoded_files(file):\n",
    "    \n",
    "    print(\"Reading file {}\".format(file))\n",
    "    with open(file, \"rb\") as reader:\n",
    "        doc_vectors = pickle.load(reader)\n",
    "        for doc in doc_vectors:\n",
    "            doc = list(doc)                \n",
    "            yield doc\n",
    "\n",
    "\n",
    "class LocalFaissRetriever():\n",
    "\n",
    "    def __init__(self): \n",
    "        self.index = DenseFlatIndexer()\n",
    "        self.index.init_index(768)\n",
    "        \n",
    "    def index_encoded_data(self, file, buffer_size):\n",
    "        buffer = []\n",
    "        for item in iterate_encoded_files(file):\n",
    "            buffer.append(item)\n",
    "            if 0 < buffer_size == len(buffer):\n",
    "                self.index.index_data(buffer)\n",
    "                buffer = []\n",
    "        self.index.index_data(buffer)\n",
    "        print(\"Data indexing completed.\")\n",
    "\n",
    "    def get_top_docs(self, query_vectors, top_docs):\n",
    "        time0 = time.time()\n",
    "        results = self.index.search_knn(query_vectors, top_docs)\n",
    "        print(\"index search time: {} sec.\".format(time.time() - time0))\n",
    "        self.index = None\n",
    "        return results\n",
    "\n",
    "\n",
    "def validate_retriever(questions, retrieved_passages):\n",
    "    \n",
    "    top = 0\n",
    "    qa_dic = {}\n",
    "      \n",
    "    # read questions\n",
    "    with open(questions, \"r\", encoding = \"utf-8\") as fin:\n",
    "         reader = csv.reader(fin, delimiter = \"\\t\")\n",
    "         for row in reader:\n",
    "             question = row[0]\n",
    "             answer = row[1].strip('\"[\"').strip('\"]\"')\n",
    "             qa_dic[question] = answer\n",
    "          \n",
    "    # read in passages from our pipeline    \n",
    "    with open(retrieved_passages, \"r\", encoding = \"utf-8\") as fin:\n",
    "       reader = csv.reader(fin, delimiter = \"\\t\")\n",
    "       for row in reader:\n",
    "           question = row[0]\n",
    "           text = row[2]        \n",
    "\n",
    "           # check if the retrieved paragraph from our pipeline is contains answer\n",
    "           if(text.find(qa_dic[question]) != -1):\n",
    "                top += 1\n",
    "              \n",
    "    print(\"Top k documents hits: {}\".format(top))\n",
    "\n",
    "\n",
    "def validate_reader(questions, span_answers):\n",
    "    \n",
    "    f1, em = [], []\n",
    "    qa_dic = {}\n",
    "    \n",
    "    with open(questions, \"r\", encoding = \"utf-8\") as fin:\n",
    "         reader = csv.reader(fin, delimiter = \"\\t\")\n",
    "         for row in reader:\n",
    "             question = row[0]\n",
    "             answer = row[1].strip('\"[\"').strip('\"]\"')\n",
    "             qa_dic[question] = answer.replace(\".\", \"\")\n",
    "             \n",
    "    with open(span_answers, \"r\", encoding = \"utf-8\") as fin:\n",
    "       reader = csv.reader(fin, delimiter = \"\\t\")\n",
    "       for row in reader:\n",
    "           question = row[0]\n",
    "           \n",
    "           answer = row[1].strip().replace(\".\", \"\").replace(\" %\", \"%\")\n",
    "           \n",
    "           macro_f1 = calculate_f1(qa_dic[question], answer)   \n",
    "           f1.append(macro_f1)\n",
    "           \n",
    "           if(macro_f1 == 1):\n",
    "               em.append(1)\n",
    "           else:\n",
    "               em.append(0)\n",
    "\n",
    " \n",
    "    print(\"em: {:.2f}, f1: {:.2f}\".format(sum(em)/len(em), sum(f1)/len(f1)))\n",
    "    \n",
    "    \n",
    "############################################## CONTEXT #############################################################\n",
    "# facebook/dpr-ctx_encoder-single-nq-base\n",
    "def generate_dense_encodings(text_corpus, ctx_encoder, out_file):\n",
    "    \n",
    "    total = 0\n",
    "    results = []\n",
    "    \n",
    "    tokenizer = DPRContextEncoderTokenizer.from_pretrained(ctx_encoder)\n",
    "    encoder = DPRContextEncoder.from_pretrained(ctx_encoder)\n",
    "\n",
    "    with open(text_corpus, encoding = \"utf-8\") as fin:\n",
    "         reader = csv.reader(fin, delimiter=\"\\t\")\n",
    "         for row in reader:\n",
    "             sample_id = str(row[0])\n",
    "             passage = row[1].strip('\"')    \n",
    "             title = row[2]\n",
    "             \n",
    "             tokens = tokenizer(title + \"[SEP]\" + passage, return_tensors=\"pt\", max_length = 256, \n",
    "                                  padding='max_length', truncation = True)[\"input_ids\"]\n",
    "                    \n",
    "             tokens[0][255] = 102            # add 102 in the end of padding  \n",
    "             embeddings = encoder(tokens).pooler_output       \n",
    "             results.extend([(sample_id, embeddings[0,:].detach().numpy())])\n",
    "             \n",
    "             total += 1\n",
    "             if(total % 10 == 0):\n",
    "                 print(\"Encoded {} passages.\".format(total))                 \n",
    "\n",
    "    with open(out_file, mode = \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "    print(\"Total passages processed {}. Written to {}\".format(len(results), out_file))\n",
    "\n",
    "\n",
    "############################################## QUESTION #############################################################\n",
    "# facebook/dpr-question_encoder-single-nq-base\n",
    "def dense_retriever(questions, question_encoder, text_corpus, corpus_embedded, retrieved_passages):\n",
    "    \n",
    "    index_buffer_sz = 50000  \n",
    "    par_dic = {}\n",
    "    \n",
    "    with open(text_corpus, \"r\", encoding = \"utf-8\") as fin:\n",
    "         reader = csv.reader(fin, delimiter=\"\\t\")\n",
    "         for row in reader:\n",
    "             sample_id = str(row[0])\n",
    "             passage = row[1].strip('\"')    \n",
    "             title = row[2]\n",
    "             \n",
    "             par_dic[sample_id] = (title, passage)\n",
    "    \n",
    "    tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(question_encoder)\n",
    "    model = DPRQuestionEncoder.from_pretrained(question_encoder)   \n",
    "\n",
    "    questions_embedded = []\n",
    "    questions_list = []\n",
    "    \n",
    "    total = 0\n",
    "    with open(retrieved_passages, \"w\", encoding = \"utf-8\") as fout:\n",
    "        with open(questions, encoding = \"utf-8\") as fin:\n",
    "             reader = csv.reader(fin, delimiter=\"\\t\")\n",
    "             for row in reader:\n",
    "                 question = row[0].strip('\"')\n",
    "                 questions_list.append(question)\n",
    "                 \n",
    "                 tokens = tokenizer(question, return_tensors=\"pt\", max_length = 256, \n",
    "                                      padding='max_length', truncation = True)[\"input_ids\"]          \n",
    "                 tokens[0][255] = 102            # add 102 in the end of padding          \n",
    "                 embeddings = model(tokens).pooler_output\n",
    "                 \n",
    "                 questions_embedded.append(embeddings[0,:])                \n",
    "                     \n",
    "                 total += 1\n",
    "                 if(total % 10 == 0):\n",
    "                     print(\"Encoded {} questions.\".format(total))                 \n",
    "                                  \n",
    "                 if(total % 100 == 0):\n",
    "    \n",
    "                     retriever = LocalFaissRetriever()\n",
    "                     retriever.index_encoded_data(corpus_embedded, index_buffer_sz)\n",
    "                 \n",
    "                     questions_embedded = torch.stack(questions_embedded)     \n",
    "                     top_results_and_scores = retriever.get_top_docs(questions_embedded.detach().numpy(), 1)\n",
    "                                                \n",
    "                     for i in range(len(top_results_and_scores)):\n",
    "                        par_id = top_results_and_scores[i][0][0]\n",
    "                            \n",
    "                        results = par_dic[par_id]\n",
    "                        title = results[0]\n",
    "                        passage = results[1]\n",
    "                            \n",
    "                        fout.write(\"{}\\t{}\\t{}\\n\".format(questions_list[i], title, passage))\n",
    "                        \n",
    "                     questions_embedded, questions_list = [], []\n",
    "                 \n",
    "\n",
    "# ############################################### READER ##############################################################\n",
    "# facebook/dpr-reader-single-nq-base\n",
    "def extractive_reader(retrieved_passages, reader, span_answers):\n",
    "\n",
    "    tokenizer = DPRReaderTokenizer.from_pretrained(reader)\n",
    "    model = DPRReader.from_pretrained(reader)\n",
    "    \n",
    "    total = 0\n",
    "    with open(span_answers, \"w\", encoding = \"utf-8\") as fout:\n",
    "        with open(retrieved_passages, \"r\", encoding = \"utf-8\") as fin:\n",
    "             reader = csv.reader(fin, delimiter=\"\\t\")\n",
    "                  \n",
    "             for row in reader:\n",
    "                 question = row[0].strip('\"')\n",
    "                 title = row[1].strip('\"')\n",
    "                 text = row[2].strip('\"')\n",
    "                 encoded_inputs = tokenizer(question, title, text, return_tensors = \"pt\", \n",
    "                        max_length = 300, padding = 'max_length', truncation = True)\n",
    "\n",
    "                 outputs = model(**encoded_inputs)\n",
    "                 predicted_spans = tokenizer.decode_best_spans(encoded_inputs, outputs, max_answer_length = 20, num_spans = 1, num_spans_per_passage = 1)\n",
    "             \n",
    "                 fout.write(\"{}\\t{}\\n\".format(question, predicted_spans[0].text))\n",
    "\n",
    "                 total += 1\n",
    "                 if(total % 10 == 0):\n",
    "                     print(\"Extracted spans for {} questions.\".format(total)) \n",
    "             \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    text_corpus = \"data/training/sleep-corpus.tsv\"\n",
    "    questions = \"data/training/sleep-test.csv\"\n",
    "    \n",
    "    ctx_encoder = \"models/pytorch/ctx_encoder/\"\n",
    "    question_encoder = \"models/pytorch/question_encoder/\"\n",
    "    reader = \"models/pytorch/reader\"\n",
    "    \n",
    "    corpus_embedded = \"models/processed/sleep-corpus_e29\"\n",
    "    retrieved_passages = \"models/processed/sleep_test_e29.csv\"\n",
    "    span_answers = \"models/processed/pipeline1_label_1.250.csv\"\n",
    "    \n",
    "    #generate_dense_encodings(text_corpus, ctx_encoder, corpus_embedded)\n",
    "    \n",
    "    #dense_retriever(questions, question_encoder, text_corpus, corpus_embedded, retrieved_passages)\n",
    "\n",
    "    #validate_retriever(questions, retrieved_passages)\n",
    " \n",
    "    extractive_reader(retrieved_passages, reader, span_answers)\n",
    "    \n",
    "    validate_reader(questions, span_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ecd0a0-d348-41ba-9d02-5b48b1b5efdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
